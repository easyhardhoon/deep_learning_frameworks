{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOY6Ct7kxjV5FQHKF1dSVRm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easyhardhoon/deep_learning_frameworks/blob/main/Pytorch5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#최적화 기법"
      ],
      "metadata": {
        "id": "gm262Nr6udr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "model = torchvision.models.resnet18(pretrained=False)"
      ],
      "metadata": {
        "id": "cjIla_jhuk0l"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**가중치 매개변수 갱신법 복습**\n",
        "\n",
        "⏰SGD : 확률적 경사 하강법. 단순히 기울어진 방향으로 일정 거리만큼 가겠다. \n",
        "\n",
        "(학습률이 고정되어있다. 방향에 따라 성질이 변하는 함수에서는 비효율적이다)\n",
        "\n",
        "⏰모멘텀 : SGD + 물리법칙. 기울어진 방향으로 가지만 물리적으로 점점 가속되는 원리\n",
        "\n",
        "⏰Adagrad : SGD + 맞춤형 학습률. 각각의 매개변수에 맞춤형 학습률만큼 SGD방식으로 가겠다.\n",
        "\n",
        "⏰Adam : 모멘텀 + Adagrad . 오늘날 가장 많이 쓰임\n",
        "\n",
        "⏰스케줄링 : 모멘텀 + 강제학습률변환. 역시 오늘날 많이 쓰임"
      ],
      "metadata": {
        "id": "U6WWqB9TwkQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. SGD**"
      ],
      "metadata": {
        "id": "69tVkaqIugR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zEkwchESuB3E"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Momentum**"
      ],
      "metadata": {
        "id": "5yPVGQqrwJbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "#그래서 pytorch4에서 위의 코드가 나온것이다. 그 코드에선 SGD방식이 아니라 momentum방식을 쓰겠다는 소리였다."
      ],
      "metadata": {
        "id": "vQfgQRMPwI5N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Adam**"
      ],
      "metadata": {
        "id": "VNWsFX66wWdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) #lr은 0.01로 정의했지만 코드를 돌리면 가변상수가 됩니다. "
      ],
      "metadata": {
        "id": "BcvJEgcrwSu2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. 스케줄링**"
      ],
      "metadata": {
        "id": "UgO7OkJE1Pmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.9) #일단 모멘텀으로 정의\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30,gamma=0.1) #학습률 강제변환. 학습률을 어떻게 바꿀것이다. \n",
        "#step기준이다. 굳이 에폭이아니라. for문의 step기준."
      ],
      "metadata": {
        "id": "tjk48Pb-wVno"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**위 코드의 의미**\n",
        "\n",
        "여기서는 일정 스텝마다 바꿈. 30에폭마다 0.1을 학습률에 곱해서 사용을 하겠다.\n",
        "\n",
        "참고로 이 방법은 StepLR이고. cos함수를 이용하는 것도 있다. cos 특성상 올렸다 내렸다가 가능하다.\n",
        "\n",
        "평원에 빠져도 다시 나올 가능성이 생긴다. 데이터에 따라 특징이 다르므로 스케쥴러는 데이터에 따라 골라서 쓴다\n",
        "\n",
        "실전에서는 파이토치 사이트에 들어가서 어떤 스케쥴러가 좋을지 비교하며 코드를 작성하면 된다\n",
        "\n",
        "Adam 자체가 학습률을 가변적으로 바꾸기 때문에 optimizer를 adam으로 하면 굳이 스케쥴러를 쓸 필요가 없다"
      ],
      "metadata": {
        "id": "RSEDroHk38ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#스케줄링 실전 사용 코드\n",
        "\"\"\"\n",
        "for epoch in range(400):\n",
        "  running_loss = 0.0\n",
        "  for data in trainloader:\n",
        "    inputs,values=data\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs,values)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  scheduler.step() #이런식입니다. 여기서는 매 epoch마다 스캐쥴러를 실행한 모습.\n",
        "\"\"\"\n",
        "#참고로. scheduler.step()을 이중 for문안에 넣어버리면 배치 기준으로 스캐쥴러가 실행되어 엄청나게 빨리\n",
        "#학습률이 바뀜.(스캐줄러의 step은 말그대로 for문의 각 step이라 항상 에폭단위가 아니다. ) 실수주의!!!\n",
        "print(\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9XDr9Tj1u58",
        "outputId": "9770f46f-6701-41ca-b0f7-64df3b991dd6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "torch.optim에서 제공하는 스케줄러 함수들입니다."
      ],
      "metadata": {
        "id": "_Tpja8B3_uI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ogFQIHlT6JPt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}